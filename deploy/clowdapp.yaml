---
apiVersion: template.openshift.io/v1
kind: Template
metadata:
  name: hive
objects:

- apiVersion: v1
  kind: ConfigMap
  metadata:
    name: hive-clowder-config
  data:
    hive-site.xml: |
      <configuration>
        <property>
          <name>hive.server2.enable.doAs</name>
          <value>false</value>
        </property>
        <property>
          <name>hive.server2.use.SSL</name>
          <value>false</value>
        </property>
        <property>
          <name>hive.server2.authentication</name>
          <value>NOSASL</value>
        </property>
        <property>
          <name>hive.metastore.metrics.enabled</name>
          <value>true</value>
        </property>
        <property>
          <name>hive.server2.metrics.enabled</name>
          <value>true</value>
        </property>
        <property>
          <name>hive.service.metrics.reporter</name>
          <value>JMX</value>
        </property>
        <property>
          <name>hive.server2.thrift.bind.host</name>
          <value>0.0.0.0</value>
        </property>
        <property>
          <name>hive.metastore.thrift.bind.host</name>
          <value>0.0.0.0</value>
        </property>
        <property>
          <name>hive.metastore.port</name>
          <value>8000</value>
        </property>
        <property>
          <name>hive.metastore.uris</name>
          <value>thrift://${HOSTNAME}:8000</value>
        </property>
        <property>
          <name>datanucleus.schema.autoCreateAll</name>
          <value>false</value>
        </property>
        <property>
          <name>hive.metastore.schema.verification</name>
          <value>false</value>
        </property>
        <property>
          <name>hive.default.fileformat</name>
          <value>Parquet</value>
        </property>
        <property>
          <name>fs.s3a.endpoint</name>
          <description>AWS S3 endpoint to connect to.</description>
          <value>XXX_S3ENDPOINT_XXX</value>
        </property>
        <property>
          <name>fs.s3a.access.key</name>
          <description>AWS access key ID.</description>
          <value>XXX_S3_ACCESS_KEY_XXX</value>
        </property>
        <property>
          <name>fs.s3a.secret.key</name>
          <description>AWS secret key.</description>
          <value>XXX_S3_SECRET_XXX</value>
        </property>
        <property>
          <name>fs.s3a.path.style.access</name>
          <value>true</value>
          <description>Enable S3 path style access.</description>
        </property>
        <property>
          <name>hive.metastore.warehouse.dir</name>
          <value>s3a://XXX_S3_BUCKET_DIR_XXX/</value>
        </property>
        <property>
          <name>hive.metastore.db.type</name>
          <value>POSTGRES</value>
          <description>
            Expects one of [derby, oracle, mysql, mssql, postgres].
            Type of database used by the metastore. Information schema &amp; JDBCStorageHandler depend on it.
          </description>
        </property>
        <property>
          <name>javax.jdo.option.ConnectionUserName</name>
          <value>XXX_DATABASE_USER_XXX</value>
          <description>Username to use against metastore database</description>
        </property>
        <property>
          <name>javax.jdo.option.ConnectionPassword</name>
          <value>XXX_DATABASE_PASSWORD_XXX</value>
          <description>password to use against metastore database</description>
        </property>
        <property>
          <name>javax.jdo.option.ConnectionURL</name>
          <value>jdbc:XXX_DATABASE_CONNECT_URL_XXX</value>
          <description>
            JDBC connect string for a JDBC metastore.
            To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL.
            For example, jdbc:postgresql://myhost/db?ssl=true for postgres database.
          </description>
        </property>
        <property>
          <name>javax.jdo.option.ConnectionDriverName</name>
          <value>org.postgresql.Driver</value>
          <description>Driver class name for a JDBC metastore</description>
        </property>
        <property>
          <name>hive.cluster.delegation.token.store.class</name>
          <value>org.apache.hadoop.hive.thrift.DBTokenStore</value>
        </property>
      </configuration>

    hive-log4j2.properties: |
      status = INFO
      name = HiveLog4j2
      packages = org.apache.hadoop.hive.ql.log

      # list of properties
      property.hive.log.level = INFO
      property.hive.root.logger = console
      property.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}
      property.hive.log.file = hive.log

      # list of all appenders
      appenders = console

      # console appender
      appender.console.type = Console
      appender.console.name = console
      appender.console.target = SYSTEM_ERR
      appender.console.layout.type = PatternLayout
      appender.console.layout.pattern = %d{yy/MM/dd HH:mm:ss} [%t]: %p %c{2}: %m%n

      # list of all loggers
      loggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX

      logger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn
      logger.NIOServerCnxn.level = WARN

      logger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO
      logger.ClientCnxnSocketNIO.level = WARN

      logger.DataNucleus.name = DataNucleus
      logger.DataNucleus.level = ERROR

      logger.Datastore.name = Datastore
      logger.Datastore.level = ERROR

      logger.JPOX.name = JPOX
      logger.JPOX.level = ERROR

      # root logger
      rootLogger.level = ${sys:hive.log.level}
      rootLogger.appenderRefs = root
      rootLogger.appenderRef.root.ref = ${sys:hive.root.logger}

    hive-exec-log4j2.properties: |
      status = INFO
      name = HiveLog4j2
      packages = org.apache.hadoop.hive.ql.log

      # list of properties
      property.hive.log.level = INFO
      property.hive.root.logger = console
      property.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}
      property.hive.log.file = hive.log

      # list of all appenders
      appenders = console

      # console appender
      appender.console.type = Console
      appender.console.name = console
      appender.console.target = SYSTEM_ERR
      appender.console.layout.type = PatternLayout
      appender.console.layout.pattern = %d{yy/MM/dd HH:mm:ss} [%t]: %p %c{2}: %m%n

      # list of all loggers
      loggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX

      logger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn
      logger.NIOServerCnxn.level = WARN

      logger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO
      logger.ClientCnxnSocketNIO.level = WARN

      logger.DataNucleus.name = DataNucleus
      logger.DataNucleus.level = ERROR

      logger.Datastore.name = Datastore
      logger.Datastore.level = ERROR

      logger.JPOX.name = JPOX
      logger.JPOX.level = ERROR

      # root logger
      rootLogger.level = ${sys:hive.log.level}
      rootLogger.appenderRefs = root
      rootLogger.appenderRef.root.ref = ${sys:hive.root.logger}

- apiVersion: v1
  kind: ConfigMap
  metadata:
    name: hive-clowder-jmx-config
    labels:
      app: hive
  data:
    config.yml: |
      ---
      lowercaseOutputName: true
      lowercaseOutputLabelNames: true
      attrNameSnakeCase: true
      whitelistObjectNames:
        - 'metrics:name=active_calls_*'
        - 'metrics:name=api_*'
        - 'metrics:name=create_*'
        - 'metrics:name=delete_*'
        - 'metrics:name=init_*'
        - 'metrics:name=exec_*'
        - 'metrics:name=hs2_*'
        - 'metrics:name=open_connections'
        - 'metrics:name=open_operations'
      rules:
        - pattern: 'metrics<name=(.*)><>Value'
          name: hive_$1
          type: GAUGE
        - pattern: 'metrics<name=(.*)><>Count'
          name: hive_$1_count
          type: GAUGE
        - pattern: 'metrics<name=(.*)><>(\d+)thPercentile'
          name: hive_$1
          type: GAUGE
          labels:
            quantile: "0.$2"

- apiVersion: v1
  kind: ConfigMap
  metadata:
    name: hive-clowder-scripts
  data:
    entrypoint.sh: |
      #!/bin/bash
      function importCert() {
        PEM_FILE=$1
        PASSWORD=$2
        KEYSTORE=$3
        # number of certs in the PEM file
        CERTS=$(grep 'END CERTIFICATE' $PEM_FILE| wc -l)

        # For every cert in the PEM file, extract it and import into the JKS keystore
        # awk command: step 1, if line is in the desired cert, print the line
        #              step 2, increment counter when last line of cert is found
        for N in $(seq 0 $(($CERTS - 1))); do
          ALIAS="${PEM_FILE%.*}-$N"
          cat $PEM_FILE |
            awk "n==$N { print }; /END CERTIFICATE/ { n++ }" |
            keytool -noprompt -import -trustcacerts \
                    -alias $ALIAS -keystore $KEYSTORE -storepass $PASSWORD
        done
      }

      set -e

      if [[ ! -z "${ACG_CONFIG}" ]]; then
        export DATABASE_HOST=$(jq -r '.database.hostname' ${ACG_CONFIG})
        export DATABASE_PORT=$(jq -r '.database.port' ${ACG_CONFIG})
        # export DATABASE_USER=$(jq -r '.database.username' ${ACG_CONFIG})
        # export DATABASE_PASSWORD=$(jq -r '.database.password' ${ACG_CONFIG})
        # export DATABASE_NAME=$(jq -r '.database.name' ${ACG_CONFIG})

        export DATABASE_SSLMODE=$(jq -r '.database.sslMode' ${ACG_CONFIG})
        if [[ $DATABASE_SSLMODE = "null" ]]; then
          unset DATABASE_SSLMODE
        fi

        certString=$(jq -r '.database.rdsCa' ${ACG_CONFIG})
        if [[ $certString != "null" ]]; then
          temp_file=$(mktemp)
          echo "RDS Cert Path: $temp_file"
          echo "$certString" > $temp_file

          export PGSSLROOTCERT=$temp_file
        fi

        export AWS_ACCESS_KEY_ID=$(jq -r '.objectStore.buckets[0].accessKey' ${ACG_CONFIG})
        export AWS_SECRET_ACCESS_KEY=$(jq -r '.objectStore.buckets[0].secretKey' ${ACG_CONFIG})
        export S3_BUCKET_NAME=$(jq -r '.objectStore.buckets[0].requestedName' ${ACG_CONFIG})

        OBJECTSTORE_HOST=$(jq -r '.objectStore.hostname' ${ACG_CONFIG})
        OBJECTSTORE_PORT=$(jq -r '.objectStore.port' ${ACG_CONFIG})
        OBJECTSTORE_TLS=$(jq -r '.objectStore.tls' ${ACG_CONFIG})

        export URI_PREFIX=https
        if [[ $OBJECTSTORE_TLS == *"false"* ]]; then
          export URI_PREFIX=http
        fi

        S3_ENDPOINT="${URI_PREFIX}://${OBJECTSTORE_HOST}"
        if [[ -n "${OBJECTSTORE_PORT}" ]] && [[ "${OBJECTSTORE_PORT}" != "null" ]]; then
          S3_ENDPOINT="${S3_ENDPOINT}:${OBJECTSTORE_PORT}"
        fi
        export S3_ENDPOINT
      fi

      # if the s3-compatible ca bundle is mounted in, add to the root Java truststore.
      if [ -a /s3-compatible-ca/ca-bundle.crt ]; then
        echo "Adding /s3-compatible-ca/ca-bundle.crt to $JAVA_HOME/lib/security/cacerts"
        importCert /s3-compatible-ca/ca-bundle.crt changeit $JAVA_HOME/lib/security/cacerts
      fi
      # always add the openshift service-ca.crt if it exists
      if [ -a /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt ]; then
        echo "Adding /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt to $JAVA_HOME/lib/security/cacerts"
        importCert /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt changeit $JAVA_HOME/lib/security/cacerts
      fi

      # add UID to /etc/passwd if missing
      if ! whoami &> /dev/null; then
          if test -w /etc/passwd || stat -c "%a" /etc/passwd | grep -qE '.[267].'; then
              echo "Adding user ${USER_NAME:-hadoop} with current UID $(id -u) to /etc/passwd"
              # Remove existing entry with user first.
              # cannot use sed -i because we do not have permission to write new
              # files into /etc
              sed  "/${USER_NAME:-hadoop}:x/d" /etc/passwd > /tmp/passwd
              # add our user with our current user ID into passwd
              echo "${USER_NAME:-hadoop}:x:$(id -u):0:${USER_NAME:-hadoop} user:${HOME}:/sbin/nologin" >> /tmp/passwd
              # overwrite existing contents with new contents (cannot replace the
              # file due to permissions)
              cat /tmp/passwd > /etc/passwd
              rm /tmp/passwd
          fi
      fi

      # symlink our configuration files to the correct location
      if [ -f /hadoop-config/core-site.xml ]; then
        cat /hadoop-config/core-site.xml | sed "s#XXX_S3ENDPOINT_XXX#${S3_BUCKET_NAME}/${S3_DATA_DIR}#" > /etc/hadoop/core-site.xml
      else
        echo "core-site.xml doesnt exist, skipping"
      fi
      if [ -f /hadoop-config/hdfs-site.xml ]; then
        ln -s -f /hadoop-config/hdfs-site.xml /etc/hadoop/hdfs-site.xml
      else
        echo "hdfs-site.xml doesnt exist, skipping symlink"
      fi

      # insert S3 bucket URI from env vars
      cat /hive-config/hive-site.xml | sed \
        -e "s#XXX_S3ENDPOINT_XXX#${S3_ENDPOINT}#" \
        -e "s#XXX_S3_ACCESS_KEY_XXX#${AWS_ACCESS_KEY_ID}#" \
        -e "s#XXX_S3_SECRET_XXX#${AWS_SECRET_ACCESS_KEY}#" \
        -e "s#XXX_S3_BUCKET_DIR_XXX#${S3_BUCKET_NAME}/${S3_DATA_DIR}#" \
        -e "s#XXX_DATABASE_USER_XXX#${DATABASE_USER}#" \
        -e "s#XXX_DATABASE_PASSWORD_XXX#${DATABASE_PASSWORD}#" \
        -e "s#XXX_DATABASE_CONNECT_URL_XXX#postgresql://${DATABASE_HOST}:${DATABASE_PORT}/${DATABASE_NAME}?sslmode=${DATABASE_SSLMODE}#" \
      > $HIVE_HOME/conf/hive-site.xml

      ln -s -f /hive-config/hive-log4j2.properties $HIVE_HOME/conf/hive-log4j2.properties
      ln -s -f /hive-config/hive-exec-log4j2.properties $HIVE_HOME/conf/hive-exec-log4j2.properties

      export HADOOP_LOG_DIR="${HADOOP_HOME}/logs"
      # Set garbage collection settings
      export GC_SETTINGS="-XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:+UseGCOverheadLimit -XX:+ExplicitGCInvokesConcurrent -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=${HADOOP_LOG_DIR}/heap_dump.bin -XX:+ExitOnOutOfMemoryError -XX:ErrorFile=${HADOOP_LOG_DIR}/java_error%p.log"
      export VM_OPTIONS="$VM_OPTIONS -XX:+UseContainerSupport"

      if [ -n "$JVM_INITIAL_RAM_PERCENTAGE" ]; then
        VM_OPTIONS="$VM_OPTIONS -XX:InitialRAMPercentage=$JVM_INITIAL_RAM_PERCENTAGE"
      fi
      if [ -n "$JVM_MAX_RAM_PERCENTAGE" ]; then
        VM_OPTIONS="$VM_OPTIONS -XX:MaxRAMPercentage=$JVM_MAX_RAM_PERCENTAGE"
      fi
      if [ -n "$JVM_MIN_RAM_PERCENTAGE" ]; then
        VM_OPTIONS="$VM_OPTIONS -XX:MinRAMPercentage=$JVM_MIN_RAM_PERCENTAGE"
      fi

      # Set JMX options
      export JMX_OPTIONS="-javaagent:/opt/jmx_exporter/jmx_exporter.jar=9000:/opt/jmx_exporter/config/config.yml -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.port=8081 -Dcom.sun.management.jmxremote.rmi.port=8081 -Djava.rmi.server.hostname=127.0.0.1"

      # Set garbage collection logs
      export GC_SETTINGS="${GC_SETTINGS} -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:${HADOOP_LOG_DIR}/gc.log"
      export GC_SETTINGS="${GC_SETTINGS} -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=5 -XX:GCLogFileSize=3M"

      export HIVE_LOGLEVEL="${HIVE_LOGLEVEL:-INFO}"
      export HADOOP_OPTS="${HADOOP_OPTS} ${VM_OPTIONS} ${GC_SETTINGS} ${JMX_OPTIONS}"
      export HIVE_METASTORE_HADOOP_OPTS=" -Dhive.log.level=${HIVE_LOGLEVEL} "
      export HIVE_OPTS="${HIVE_OPTS} --hiveconf hive.root.logger=${HIVE_LOGLEVEL},console "

      set +e
      if schematool -dbType postgres -info -verbose; then
          echo "Hive metastore schema verified."
      else
          if schematool -dbType postgres -initSchema -verbose; then
              echo "Hive metastore schema created."
          else
              echo "Error creating hive metastore: $?"
          fi
      fi
      set -e

      exec $@


- apiVersion: v1
  kind: ConfigMap
  metadata:
    name: hadoop-clowder-config
  data:
    core-site.xml: |
      <configuration>
        <property>
            <name>fs.defaultFS</name>
            <value>s3a://XXX_S3ENDPOINT_XXX/</value>
        </property>
        <property>
            <name>fs.gs.impl</name>
            <value>com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem</value>
        </property>
        <property>
            <name>fs.AbstractFileSystem.wasb.Impl</name>
            <value>org.apache.hadoop.fs.azure.Wasb</value>
        </property>
        <property>
            <name>fs.AbstractFileSystem.gs.impl</name>
            <value>com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS</value>
        </property>
        <property>
            <name>fs.gs.auth.service.account.enable</name>
            <value>true</value>
        </property>
        <property>
            <name>fs.gs.reported.permissions</name>
            <value>733</value>
        </property>
      </configuration>

- apiVersion: cloud.redhat.com/v1alpha1
  kind: ClowdApp
  metadata:
    name: hive
  spec:
    envName: ${ENV_NAME}
    deployments:
    - name: metastore
      minReplicas: ${{MIN_REPLICAS}}
      webServices:
        public:
          enabled: true
        metrics:
          enabled: true
      podSpec:
        image: ${IMAGE}:${IMAGE_TAG}
        command: ["/hive-scripts/entrypoint.sh"]
        args: ["/opt/hive/bin/hive", "--service", "metastore"]
        livenessProbe:
          failureThreshold: 3
          tcpSocket:
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        readinessProbe:
          httpGet:
            path: /
            port: 9000
            scheme: HTTP
          initialDelaySeconds: 60
          periodSeconds: 10
          successThreshold: 1
          failureThreshold: 3
          timeoutSeconds: 5
        env:
        - name: HIVE_LOGLEVEL
          value: INFO
        - name: S3_BUCKET_NAME
          value: ${S3_BUCKET_NAME}
        - name: S3_DATA_DIR
          value: 'data'
        - name: DATABASE_SSLMODE
          value: ${DATABASE_SSLMODE}
        - name: DATABASE_USER
          valueFrom:
            secretKeyRef:
              name: hive-db
              key: database_user
        - name: DATABASE_PASSWORD
          valueFrom:
            secretKeyRef:
              name: hive-db
              key: database_password
        - name: DATABASE_NAME
          valueFrom:
            secretKeyRef:
              name: hive-db
              key: database_name
        - name: MY_MEM_REQUEST
          valueFrom:
            resourceFieldRef:
              containerName: hive-metastore
              resource: requests.memory
        - name: MY_MEM_LIMIT
          valueFrom:
            resourceFieldRef:
              containerName: hive-metastore
              resource: limits.memory
        resources:
          requests:
            cpu: ${CPU_REQUEST}
            memory: ${MEMORY_REQUEST}
          limits:
            cpu: ${CPU_LIMIT}
            memory: ${MEMORY_LIMIT}
        volumes:
        - name: hive-config
          configMap:
            name: hive-clowder-config
        - name: hive-scripts
          configMap:
            name: hive-clowder-scripts
            defaultMode: 0775
        - name: hive-jmx-config
          configMap:
            name: hive-clowder-jmx-config
        - name: namenode-empty
          emptyDir: {}
        - name: datanode-empty
          emptyDir: {}
        - name: hadoop-logs
          emptyDir: {}
        - name: hive-metastore-db-data
          emptyDir: {}
        - name: hadoop-config
          configMap:
            name: hadoop-clowder-config
            defaultMode: 420
        volumeMounts:
        - name: hive-config
          mountPath: /hive-config
        - name: hive-scripts
          mountPath: /hive-scripts
        - name: hive-jmx-config
          mountPath: /opt/jmx_exporter/config
        - name: hive-metastore-db-data
          mountPath: /var/lib/hive
        - name: namenode-empty
          mountPath: /hadoop/dfs/name
        - name: datanode-empty
          mountPath: /hadoop/dfs/data
        - name: hadoop-logs
          mountPath: /opt/hadoop/logs
        - name: hadoop-config
          mountPath: /hadoop-config
    objectStore:
    - ${S3_BUCKET_NAME}
    database:
      sharedDbAppName: koku
    dependencies:
      - koku

- apiVersion: v1
  kind: Secret # For ephemeral/local environment only
  metadata:
    name: hive-db
  stringData:
    database_name: "${HIVE_DB_NAME_EPH}"
    database_user: "${HIVE_DB_USER_EPH}"
    database_password: "${HIVE_DB_PASSWORD_EPH}"

parameters:
- description: ClowdEnv Name
  name: ENV_NAME
  requred: false
- name: MIN_REPLICAS
  value: "1"
- description: Initial amount of memory the container will request.
  displayName: Memory Request
  name: MEMORY_REQUEST
  required: true
  value: 300Mi
- description: Maximum amount of memory the container can use.
  displayName: Memory Limit
  name: MEMORY_LIMIT
  required: true
  value: 600Mi
- description: Initial amount of CPU the container will request.
  displayName: CPU Request
  name: CPU_REQUEST
  required: true
  value: 250m
- description: Maximum amount of CPU the container can use.
  displayName: CPU Limit
  name: CPU_LIMIT
  required: true
  value: 500m
- description: Whether to connect to the database using SSL/TLS; valid values are "true" or "false"
  displayName: Database uses SSL/TLS
  name: DATABASE_SSLMODE
  required: true
  value: "prefer"
- description: Image name
  name: IMAGE
  value: quay.io/cloudservices/ubi-hive
- description: Image tag
  displayName: Image tag
  name: IMAGE_TAG
  value: '2.3.3-002'
  required: true
- name: S3_BUCKET_NAME
  value: 'hive-s3'

- name: HIVE_DB_NAME_EPH
  value: hive
- name: HIVE_DB_USER_EPH
  value: hive
- name: HIVE_DB_PASSWORD_EPH
  value: hive
