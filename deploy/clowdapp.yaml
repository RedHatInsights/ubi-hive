---
apiVersion: template.openshift.io/v1
kind: Template
metadata:
  name: hive
objects:

- apiVersion: v1
  kind: ConfigMap
  metadata:
    name: metastore-clowder-config
  data:
    metastore-site.xml: |
      <configuration>
        <property>
          <name>metastore.thrift.port</name>
          <value>10000</value>
          <description>Hive metastore listener port</description>
        </property>
        <property>
          <name>metastore.thrift.uris</name>
          <value>thrift://${HOSTNAME}:10000</value>
          <description>Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.</description>
        </property>
        <property>
          <name>metastore.metrics.enabled</name>
          <value>true</value>
          <description>Enable metrics on the metastore.</description>
        </property>
        <property>
          <name>metastore.metrics.reporters</name>
          <value>jmx</value>
          <description>A comma separated list of metrics reporters to start</description>
        </property>
        <property>
          <name>datanucleus.autoStartMechanismMode</name>
          <value>ignored</value>
          <description>Autostart mechanism for datanucleus.  Currently ignored is the only option supported.</description>
        </property>
        <property>
          <name>datanucleus.schema.autoCreateAll</name>
          <value>false</value>
          <description>Auto creates necessary schema on a startup if one doesn't exist. Set this to false, after creating it once.To enable auto create also set hive.metastore.schema.verification=false. Auto creation is not recommended for production use cases, run schematool command instead.</description>
        </property>
        <property>
          <name>datanucleus.connectionPool.maxPoolSize</name>
          <value>8</value>
          <description>Specify the maximum number of connections in the connection pool.</description>
        </property>
        <property>
          <name>hive.metastore.schema.verification</name>
          <value>true</value>
          <description>
            Enforce metastore schema version consistency.
            True: Verify that version information stored in is compatible with one from Hive jars.  Also disable automatic
                  schema migration attempt. Users are required to manually migrate schema after Hive upgrade which ensures
                  proper metastore schema migration. (Default)
            False: Warn if the version information stored in metastore doesn't match with one from in Hive jars.
          </description>
        </property>
        <property>
          <name>hive.default.fileformat</name>
          <value>Parquet</value>
        </property>
        <property>
          <name>fs.s3a.endpoint</name>
          <description>AWS S3 endpoint to connect to.</description>
          <value>XXX_S3ENDPOINT_XXX</value>
        </property>
        <property>
          <name>fs.s3a.access.key</name>
          <description>AWS access key ID.</description>
          <value>XXX_S3_ACCESS_KEY_XXX</value>
        </property>
        <property>
          <name>fs.s3a.secret.key</name>
          <description>AWS secret key.</description>
          <value>XXX_S3_SECRET_XXX</value>
        </property>
        <property>
          <name>fs.s3a.path.style.access</name>
          <value>true</value>
          <description>Enable S3 path style access.</description>
        </property>
        <property>
          <name>metastore.warehouse.dir</name>
          <value>s3a://XXX_S3_BUCKET_DIR_XXX/</value>
        </property>
        <property>
          <name>hive.metastore.db.type</name>
          <value>POSTGRES</value>
          <description>
            Expects one of [derby, oracle, mysql, mssql, postgres].
            Type of database used by the metastore. Information schema &amp; JDBCStorageHandler depend on it.
          </description>
        </property>
        <property>
          <name>javax.jdo.option.ConnectionUserName</name>
          <value>XXX_DATABASE_USER_XXX</value>
          <description>Username to use against metastore database</description>
        </property>
        <property>
          <name>javax.jdo.option.ConnectionPassword</name>
          <value>XXX_DATABASE_PASSWORD_XXX</value>
          <description>password to use against metastore database</description>
        </property>
        <property>
          <name>javax.jdo.option.ConnectionURL</name>
          <value>jdbc:XXX_DATABASE_CONNECT_URL_XXX</value>
          <description>
            JDBC connect string for a JDBC metastore.
            To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL.
            For example, jdbc:postgresql://myhost/db?ssl=true for postgres database.
          </description>
        </property>
        <property>
          <name>javax.jdo.option.ConnectionDriverName</name>
          <value>org.postgresql.Driver</value>
          <description>Driver class name for a JDBC metastore</description>
        </property>
        <property>
          <name>hive.cluster.delegation.token.store.class</name>
          <value>org.apache.hadoop.hive.thrift.DBTokenStore</value>
        </property>
        <property>
          <name>metastore.task.threads.always</name>
          <value>org.apache.hadoop.hive.metastore.events.EventCleanerTask</value>
        </property>
        <property>
          <name>metastore.expression.proxy</name>
          <value>org.apache.hadoop.hive.metastore.DefaultPartitionExpressionProxy</value>
        </property>
      </configuration>

    metastore-log4j2.properties: |
      status = INFO
      name = MetastoreLog4j2
      packages = org.apache.hadoop.hive.metastore

      # list of properties
      property.metastore.log.level = INFO
      property.metastore.root.logger = console
      property.metastore.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}
      property.metastore.log.file = metastore.log

      # list of all appenders
      appenders = console

      # console appender
      appender.console.type = Console
      appender.console.name = console
      appender.console.target = SYSTEM_ERR
      appender.console.layout.type = PatternLayout
      appender.console.layout.pattern = %d{yy/MM/dd HH:mm:ss} [%t]: %p %c{2}: %m%n

      # list of all loggers
      loggers = DataNucleus, Datastore, JPOX

      logger.DataNucleus.name = DataNucleus
      logger.DataNucleus.level = ERROR

      logger.Datastore.name = Datastore
      logger.Datastore.level = ERROR

      logger.JPOX.name = JPOX
      logger.JPOX.level = ERROR

      # root logger
      rootLogger.level = ${sys:metastore.log.level}
      rootLogger.appenderRefs = root
      rootLogger.appenderRef.root.ref = ${sys:metastore.root.logger}

    jmx-config.yaml: |
      ---
      lowercaseOutputName: true
      lowercaseOutputLabelNames: true
      attrNameSnakeCase: true
      whitelistObjectNames:
        - 'metrics:name=active_calls_*'
        - 'metrics:name=api_*'
        - 'metrics:name=create_*'
        - 'metrics:name=delete_*'
        - 'metrics:name=init_*'
        - 'metrics:name=exec_*'
        - 'metrics:name=hs2_*'
        - 'metrics:name=open_connections'
        - 'metrics:name=open_operations'
      rules:
        - pattern: 'metrics<name=(.*)><>Value'
          name: hive_$1
          type: GAUGE
        - pattern: 'metrics<name=(.*)><>Count'
          name: hive_$1_count
          type: GAUGE
        - pattern: 'metrics<name=(.*)><>(\d+)thPercentile'
          name: hive_$1
          type: GAUGE
          labels:
            quantile: "0.$2"

- apiVersion: v1
  kind: ConfigMap
  metadata:
    name: metastore-clowder-scripts
  data:
    entrypoint.sh: |
      #!/bin/bash

      export PATH=${METASTORE_HOME}/bin:$PATH

      function importCert() {
        PEM_FILE=$1
        PASSWORD=$2
        KEYSTORE=$3
        # number of certs in the PEM file
        CERTS=$(grep 'END CERTIFICATE' $PEM_FILE| wc -l)

        # For every cert in the PEM file, extract it and import into the JKS keystore
        # awk command: step 1, if line is in the desired cert, print the line
        #              step 2, increment counter when last line of cert is found
        for N in $(seq 0 $(($CERTS - 1))); do
          ALIAS="${PEM_FILE%.*}-$N"
          cat $PEM_FILE |
            awk "n==$N { print }; /END CERTIFICATE/ { n++ }" |
            keytool -noprompt -import -trustcacerts \
                    -alias $ALIAS -keystore $KEYSTORE -storepass $PASSWORD
        done
      }

      set -e

      if [[ ! -z "${ACG_CONFIG}" ]]; then
        export DATABASE_HOST=$(jq -r '.database.hostname' ${ACG_CONFIG})
        export DATABASE_PORT=$(jq -r '.database.port' ${ACG_CONFIG})
        # export DATABASE_USER=$(jq -r '.database.username' ${ACG_CONFIG})
        # export DATABASE_PASSWORD=$(jq -r '.database.password' ${ACG_CONFIG})
        # export DATABASE_NAME=$(jq -r '.database.name' ${ACG_CONFIG})

        export DATABASE_SSLMODE=$(jq -r '.database.sslMode' ${ACG_CONFIG})
        if [[ $DATABASE_SSLMODE = "null" ]]; then
          unset DATABASE_SSLMODE
        fi

        certString=$(jq -r '.database.rdsCa' ${ACG_CONFIG})
        if [[ $certString != "null" ]]; then
          temp_file=$(mktemp)
          echo "RDS Cert Path: $temp_file"
          echo "$certString" > $temp_file

          export PGSSLROOTCERT=$temp_file
        fi

        export AWS_ACCESS_KEY_ID=$(jq -r '.objectStore.buckets[0].accessKey' ${ACG_CONFIG})
        export AWS_SECRET_ACCESS_KEY=$(jq -r '.objectStore.buckets[0].secretKey' ${ACG_CONFIG})
        export S3_BUCKET_NAME=$(jq -r '.objectStore.buckets[0].requestedName' ${ACG_CONFIG})

        OBJECTSTORE_HOST=$(jq -r '.objectStore.hostname' ${ACG_CONFIG})
        OBJECTSTORE_PORT=$(jq -r '.objectStore.port' ${ACG_CONFIG})
        OBJECTSTORE_TLS=$(jq -r '.objectStore.tls' ${ACG_CONFIG})

        export URI_PREFIX=https
        if [[ $OBJECTSTORE_TLS == *"false"* ]]; then
          export URI_PREFIX=http
        fi

        S3_ENDPOINT="${URI_PREFIX}://${OBJECTSTORE_HOST}"
        if [[ -n "${OBJECTSTORE_PORT}" ]] && [[ "${OBJECTSTORE_PORT}" != "null" ]]; then
          S3_ENDPOINT="${S3_ENDPOINT}:${OBJECTSTORE_PORT}"
        fi
        export S3_ENDPOINT
      fi

      export JAVA_HOME=/usr/lib/jvm/jre-1.8.0-openjdk
      # if the s3-compatible ca bundle is mounted in, add to the root Java truststore.
      if [ -a /s3-compatible-ca/ca-bundle.crt ]; then
        echo "Adding /s3-compatible-ca/ca-bundle.crt to $JAVA_HOME/lib/security/cacerts"
        importCert /s3-compatible-ca/ca-bundle.crt changeit $JAVA_HOME/lib/security/cacerts
      fi
      # always add the openshift service-ca.crt if it exists
      if [ -a /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt ]; then
        echo "Adding /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt to $JAVA_HOME/lib/security/cacerts"
        importCert /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt changeit $JAVA_HOME/lib/security/cacerts
      fi

      # add UID to /etc/passwd if missing
      if ! whoami &> /dev/null; then
          if test -w /etc/passwd || stat -c "%a" /etc/passwd | grep -qE '.[267].'; then
              echo "Adding user ${USER_NAME:-hadoop} with current UID $(id -u) to /etc/passwd"
              # Remove existing entry with user first.
              # cannot use sed -i because we do not have permission to write new
              # files into /etc
              sed  "/${USER_NAME:-hadoop}:x/d" /etc/passwd > /tmp/passwd
              # add our user with our current user ID into passwd
              echo "${USER_NAME:-hadoop}:x:$(id -u):0:${USER_NAME:-hadoop} user:${HOME}:/sbin/nologin" >> /tmp/passwd
              # overwrite existing contents with new contents (cannot replace the
              # file due to permissions)
              cat /tmp/passwd > /etc/passwd
              rm /tmp/passwd
          fi
      fi

      # insert S3 bucket URI from env vars
      cat /metastore-config/metastore-site.xml | sed \
        -e "s#XXX_S3ENDPOINT_XXX#${S3_ENDPOINT}#" \
        -e "s#XXX_S3_ACCESS_KEY_XXX#${AWS_ACCESS_KEY_ID}#" \
        -e "s#XXX_S3_SECRET_XXX#${AWS_SECRET_ACCESS_KEY}#" \
        -e "s#XXX_S3_BUCKET_DIR_XXX#${S3_BUCKET_NAME}/${S3_DATA_DIR}#" \
        -e "s#XXX_DATABASE_USER_XXX#${DATABASE_USER}#" \
        -e "s#XXX_DATABASE_PASSWORD_XXX#${DATABASE_PASSWORD}#" \
        -e "s#XXX_DATABASE_CONNECT_URL_XXX#postgresql://${DATABASE_HOST}:${DATABASE_PORT}/${DATABASE_NAME}?sslmode=${DATABASE_SSLMODE}#" \
      > $METASTORE_HOME/conf/metastore-site.xml

      ln -s -f /metastore-config/metastore-log4j2.properties $METASTORE_HOME/conf/metastore-log4j2.properties
      ln -s -f /metastore-config/jmx-config.yaml $METASTORE_HOME/conf/jmx-config.yaml

      # Set garbage collection settings
      export GC_SETTINGS="-XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:+UseGCOverheadLimit -XX:+ExplicitGCInvokesConcurrent -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=${HADOOP_HOME}/logs/heap_dump.bin -XX:+ExitOnOutOfMemoryError -XX:ErrorFile=${HADOOP_HOME}/logs/java_error%p.log"
      export VM_OPTIONS="$VM_OPTIONS -Xmx$MAX_HEAP_SIZE -XshowSettings:VM -XX:+UseContainerSupport"

      if [ -n "$JVM_INITIAL_RAM_PERCENTAGE" ]; then
        VM_OPTIONS="$VM_OPTIONS -XX:InitialRAMPercentage=$JVM_INITIAL_RAM_PERCENTAGE"
      fi
      if [ -n "$JVM_MAX_RAM_PERCENTAGE" ]; then
        VM_OPTIONS="$VM_OPTIONS -XX:MaxRAMPercentage=$JVM_MAX_RAM_PERCENTAGE"
      fi
      if [ -n "$JVM_MIN_RAM_PERCENTAGE" ]; then
        VM_OPTIONS="$VM_OPTIONS -XX:MinRAMPercentage=$JVM_MIN_RAM_PERCENTAGE"
      fi

      # Set JMX options
      export JMX_OPTIONS="-javaagent:${METASTORE_HOME}/lib/jmx_exporter.jar=9000:${METASTORE_HOME}/conf/jmx-config.yaml"

      # Set garbage collection logs
      export GC_SETTINGS="${GC_SETTINGS} -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:${HADOOP_HOME}/logs/gc.log"
      export GC_SETTINGS="${GC_SETTINGS} -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=5 -XX:GCLogFileSize=3M"

      export HADOOP_CLASSPATH=${HADOOP_HOME}/share/hadoop/tools/lib/aws-java-sdk-bundle-1.11.375.jar:${HADOOP_HOME}/share/hadoop/tools/lib/hadoop-aws-${HADOOP_VERSION}.jar
      export HIVE_LOGLEVEL="${HIVE_LOGLEVEL:-INFO}"
      export HADOOP_OPTS="${HADOOP_OPTS} ${VM_OPTIONS} ${GC_SETTINGS} ${JMX_OPTIONS}"
      export HIVE_METASTORE_HADOOP_OPTS=" -Dhive.log.level=${HIVE_LOGLEVEL} "
      export HIVE_OPTS="${HIVE_OPTS} --hiveconf metastore.root.logger=${HIVE_LOGLEVEL},console "
      export HADOOP_CLIENT_OPTS="$HADOOP_CLIENT_OPTS -Dlog4j2.formatMsgNoLookups=true"

      set +e
      if schematool -dbType postgres -info -verbose; then
          echo "Hive metastore schema verified."
      else
          if schematool -dbType postgres -initSchema -verbose; then
              echo "Hive metastore schema created."
          else
              echo "Error creating hive metastore: $?"
          fi
      fi
      set -e

      start-metastore

- apiVersion: v1
  kind: ConfigMap
  metadata:
    name: hadoop-clowder-config
  data:
    core-site.xml: |
      <configuration>
        <property>
            <name>fs.defaultFS</name>
            <value>s3a://XXX_S3ENDPOINT_XXX/</value>
        </property>
        <property>
            <name>fs.gs.impl</name>
            <value>com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem</value>
        </property>
        <property>
            <name>fs.AbstractFileSystem.wasb.Impl</name>
            <value>org.apache.hadoop.fs.azure.Wasb</value>
        </property>
        <property>
            <name>fs.AbstractFileSystem.gs.impl</name>
            <value>com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS</value>
        </property>
        <property>
            <name>fs.gs.auth.service.account.enable</name>
            <value>true</value>
        </property>
        <property>
            <name>fs.gs.reported.permissions</name>
            <value>733</value>
        </property>
      </configuration>

- apiVersion: cloud.redhat.com/v1alpha1
  kind: ClowdApp
  metadata:
    name: hive
    annotations:
      ignore-check.kube-linter.io/minimum-three-replicas: This deployment uses 1 pod as currently the metastore is a singleton
  spec:
    envName: ${ENV_NAME}
    deployments:
    - name: metastore
      metadata:
        annotations:
          ignore-check.kube-linter.io/minimum-three-replicas: This deployment uses 1 pod as currently the metastore is a singleton
      minReplicas: ${{MIN_REPLICAS}}
      webServices:
        public:
          enabled: false
        private:
          enabled: true
        metrics:
          enabled: true
      podSpec:
        image: ${IMAGE}:${IMAGE_TAG}
        command: ["/metastore-scripts/entrypoint.sh"]
        livenessProbe:
          failureThreshold: 3
          tcpSocket:
            port: 10000
          initialDelaySeconds: 60
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        readinessProbe:
          httpGet:
            path: /
            port: 9000
            scheme: HTTP
          initialDelaySeconds: 60
          periodSeconds: 10
          successThreshold: 1
          failureThreshold: 3
          timeoutSeconds: 5
        env:
        - name: HIVE_LOGLEVEL
          value: INFO
        - name: S3_BUCKET_NAME
          value: ${S3_BUCKET_NAME}
        - name: S3_DATA_DIR
          value: 'data'
        - name: DATABASE_SSLMODE
          value: ${DATABASE_SSLMODE}
        - name: DATABASE_USER
          valueFrom:
            secretKeyRef:
              name: hive-db
              key: database_user
        - name: DATABASE_PASSWORD
          valueFrom:
            secretKeyRef:
              name: hive-db
              key: database_password
        - name: DATABASE_NAME
          valueFrom:
            secretKeyRef:
              name: hive-db
              key: database_name
        - name: MY_MEM_REQUEST
          valueFrom:
            resourceFieldRef:
              containerName: hive-metastore
              resource: requests.memory
        - name: MY_MEM_LIMIT
          valueFrom:
            resourceFieldRef:
              containerName: hive-metastore
              resource: limits.memory
        - name: MAX_HEAP_SIZE
          value: ${MAX_HEAP_SIZE}
        resources:
          requests:
            cpu: ${CPU_REQUEST}
            memory: ${MEMORY_REQUEST}
          limits:
            cpu: ${CPU_LIMIT}
            memory: ${MEMORY_LIMIT}
        volumes:
        - name: metastore-conf
          emptyDir: {}
        - name: metastore-config
          configMap:
            name: metastore-clowder-config
        - name: metastore-scripts
          configMap:
            name: metastore-clowder-scripts
            defaultMode: 0775
        - name: hadoop-logs
          emptyDir: {}
        - name: hadoop-config
          configMap:
            name: hadoop-clowder-config
            defaultMode: 420
        volumeMounts:
        - name: metastore-conf
          mountPath: /opt/hive-metastore-bin/conf
        - name: metastore-config
          mountPath: /metastore-config
        - name: metastore-scripts
          mountPath: /metastore-scripts
        - name: hadoop-logs
          mountPath: /opt/hadoop/logs
        - name: hadoop-config
          mountPath: /hadoop-config
    objectStore:
    - ${S3_BUCKET_NAME}
    database:
      sharedDbAppName: koku
    dependencies:
      - koku

- apiVersion: v1
  kind: Secret # For ephemeral/local environment only
  metadata:
    name: hive-db
  stringData:
    database_name: "${HIVE_DB_NAME_EPH}"
    database_user: "${HIVE_DB_USER_EPH}"
    database_password: "${HIVE_DB_PASSWORD_EPH}"

parameters:
- description: ClowdEnv Name
  name: ENV_NAME
  requred: false
- name: MIN_REPLICAS
  value: "1"
- description: maximum heap size
  displayName: xmx
  name: MAX_HEAP_SIZE
  value: '512M'
- description: Initial amount of memory the container will request.
  displayName: Memory Request
  name: MEMORY_REQUEST
  required: true
  value: 1Gi
- description: Maximum amount of memory the container can use.
  displayName: Memory Limit
  name: MEMORY_LIMIT
  required: true
  value: 2Gi
- description: Initial amount of CPU the container will request.
  displayName: CPU Request
  name: CPU_REQUEST
  required: true
  value: 250m
- description: Maximum amount of CPU the container can use.
  displayName: CPU Limit
  name: CPU_LIMIT
  required: true
  value: 500m
- description: Whether to connect to the database using SSL/TLS; valid values are "true" or "false"
  displayName: Database uses SSL/TLS
  name: DATABASE_SSLMODE
  required: true
  value: "prefer"
- description: Image name
  name: IMAGE
  value: quay.io/cloudservices/ubi-hive
- description: Image tag
  displayName: Image tag
  name: IMAGE_TAG
  value: '2.3.3-002'
  required: true
- name: S3_BUCKET_NAME
  value: 'hive-s3'

- name: HIVE_DB_NAME_EPH
  value: hive
- name: HIVE_DB_USER_EPH
  value: hive
- name: HIVE_DB_PASSWORD_EPH
  value: hive
